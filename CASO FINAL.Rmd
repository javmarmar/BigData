---
title: "Caso Pŕactico Final"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---

Tomaremos el dataset de aprobación de crédito bancario en https://archive.ics.uci.edu/ml/datasets/Credit+Approval . Los datos también se pueden cargar de la carpeta de contenido en  `crx.data`. La información del dataset está en https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.names y expone lo siguiente:

      1. Title: Credit Approval

      2. Sources: 
          (confidential)
          Submitted by quinlan@cs.su.oz.au
      
      3.  Past Usage:
      
          See Quinlan,
          * "Simplifying decision trees", Int J Man-Machine Studies 27,
            Dec 1987, pp. 221-234.
          * "C4.5: Programs for Machine Learning", Morgan Kaufmann, Oct 1992
        
      4.  Relevant Information:
      
          This file concerns credit card applications.  All attribute names
          and values have been changed to meaningless symbols to protect
          confidentiality of the data.
        
          This dataset is interesting because there is a good mix of
          attributes -- continuous, nominal with small numbers of
          values, and nominal with larger numbers of values.  There
          are also a few missing values.
        
      5.  Number of Instances: 690
      
      6.  Number of Attributes: 15 + class attribute
      
      7.  Attribute Information:
      
          A1:	b, a.
          A2:	continuous.
          A3:	continuous.
          A4:	u, y, l, t.
          A5:	g, p, gg.
          A6:	c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff.
          A7:	v, h, bb, j, n, z, dd, ff, o.
          A8:	continuous.
          A9:	t, f.
          A10:	t, f.
          A11:	continuous.
          A12:	t, f.
          A13:	g, p, s.
          A14:	continuous.
          A15:	continuous.
          A16: +,-         (class attribute)
      
      8.  Missing Attribute Values:
          37 cases (5%) have one or more missing values.  The missing
          values from particular attributes are:
      
          A1:  12
          A2:  12
          A4:   6
          A5:   6
          A6:   9
          A7:   9
          A14: 13
      
      9.  Class Distribution
        
          +: 307 (44.5%)
          -: 383 (55.5%)
      
## Actividades a realizar

1. Carga los datos. Realiza una inspección por variables de la distribución de aprobación de crédito en función de cada atributo visualmente. Realiza las observaciones pertinentes. ¿ Qué variables son mejores para separar los datos?
2. Prepara el dataset convenientemente e imputa los valores faltantes usando la librería `missForest`.
```{r}
library(dplyr)
library(randomForest)
library(missForest)
```
En primer lugar cargamos los datos leyendo el fichero 'caso_final.csv' proporcionado ; lo hacemos sin cabecera y los valores no proporcionados , que aparecen en el fichero con el carácter '?' los reconocemos como NA.Nombramos asimismo las columnas correspondientes a cada variable: 

```{r}
df <- read.csv('caso_final.csv',header=FALSE,na.strings  = '?')
colnames(df) <- c("A1","A2","A3","A4","A5","A6","A7","A8","A9","A10","A11","A12","A13","A14","A15","A16")
df
```
Comprobamos el número de valores 'NA' del fichero por variable:

```{r}
apply(is.na(df),2,sum)
```
Puesto que tenemos valores faltantes, los imputamos usando el método MissForest de la librería del mismo nombre y comprobamos que después de las iteraciones correspondientes ya no tenemos valores faltantes en el archivo de datos:
```{r}
df.imp <- missForest(df,maxiter = 20,ntree = 500,variablewise = T)
df.imp$OOBerror
apply(df,2,var,na.rm=TRUE)
apply(is.na(df.imp$ximp),2,sum)
```
Modificamos el fichero de datos; puesto que la variable respuesta es la variable factor A16 correspondiente a la aprobación o no de crédito y pensamos en realizar una regresión logística , transformamos la variable a numérica con valores 0 o 1 (correspondientes a aprobación o no del crédito respectivamente),eliminamos la variable atributo antigua del dataset y definimos la variable Aprob como nueva variable respuesta.Exploramos los datos separados por variables y observamos que tenemos variables numéricas y de factor como variables regresoras :
```{r}
df <- df.imp$ximp
A17<-as.numeric(df$A16)-1
df$Aprob<-A17
df<-select(df,-A16)
str(df)
summary(df)
```
Pintamos gráficos de cada una de las variables regresoras respecto a la variable respuesta con el fin de conseguir una aproximación visual de la influencia de cada una de ellas en la aprobación de crédito:

```{r}
plot(Aprob ~ A1, data=df)
plot(Aprob ~ A2, data=df)
plot(Aprob ~ A3, data=df)
plot(Aprob ~ A4, data=df)
plot(Aprob ~ A5, data=df)
plot(Aprob ~ A6, data=df)
plot(Aprob ~ A7, data=df)
plot(Aprob ~ A8, data=df)
plot(Aprob ~ A9, data=df)
plot(Aprob ~ A10, data=df)
plot(Aprob ~ A11, data=df)
plot(Aprob ~ A12, data=df)
plot(Aprob ~ A13, data=df)
plot(Aprob ~ A14, data=df)
plot(Aprob ~ A15, data=df)
```
Podemos observar a través de los gráficos que las variables factor permiten separar más o menos bien los datos ( en particular las variables A9 y A10 ) , mientras que las numéricas , aunque influyentes en mayor o menor medida , no permiten separar realmente bien los datos , por lo que agrupamos las variables numéricas con la variable respuesta y calculamos la matriz de correlaciones para ver si nos aporta alguna situación interesante :

```{r}
numericas<-select(df,A2,A3,A8,A11,A14,A15,Aprob)
numericas
```
```{r}
library(Hmisc)
cor(numericas)
```
Como podemos ver en la matriz de correlaciones , ninguna de las variables numéricas tiene correlación a tener en cuenta con la variable respuesta ( salvo quizás la variable A11 que está directamente correlada al 40% con la variable respuesta)

3. Divide el dataset tomando las primeras 590 instancias como train y las últimas 100 como test.

Creamos la matriz de predictores y vector de la variable objetivo:
```{r}
X <- data.matrix(subset(df,select= - Aprob))
y <- as.double(as.matrix(df$Aprob))
str(X)
```
Vamos a dividir en dos conjuntos, uno de entrenamiento y otro de test con 590 y 100 elementos en cada uno respectivamente como se nos pide, para evaluar la capacidad predictiva de los posibles modelos:
```{r}
dim(X)
length(y)
X_train <- X[1:590,]
y_train <- y[1:590]
X_test <- X[591:690,]
y_test <- y[591:690]
```
4. Entrena un modelo de regresión logística con regularización Ridge y Lasso en train seleccionando el que mejor **AUC** tenga. Da las métricas en test.

Establecemos primero todo lo que se nos pide para el modelo Ridge,en el que el valor natural de los coeficientes es 0, penalizando la adjudicación de valor. En este modelo no se obtienen coeficientes finales nulos, aunque sean muy pequeños. Es, por tanto, un modelo que regulariza de manera continua todos los coeficientes.

MODELO DE REGRESIÓN LOGÍSTICA CON REGULARIZACIÓN RIDGE :

Cargamos la librería glmnet para estos pasos y programamos el modelo a realizar estableciendo medida "AUC" y con dataset de entrenamiento :

```{r}
library(glmnet)
set.seed(999)
cv.ridge <- cv.glmnet(X_train, y_train, family='binomial', alpha=0, parallel=TRUE, standardize=TRUE, type.measure='auc')
# Resultados
plot(cv.ridge)
#este es el mejor valor de lambda
cv.ridge$lambda.min
#este es el valor del error que se estima para ese valor lambda mínimo dado en MSE
max(cv.ridge$cvm)
```
Vemos los coeficientes del modelo obtenido:

```{r}
coef(cv.ridge, s=cv.ridge$lambda.min)
```
Como podemos observar , además de la variable intercepto , solo las variables A9 y A10 (en menor medida) parecen tener relevancia a tenor de los coeficientes obtenidos en el modelo.

Damos métricas para los valores test y obtenemos la correspondiente matriz de confusión para dichos valores:
```{r}
y_pred <- as.numeric(predict.glmnet(cv.ridge$glmnet.fit, newx=X_test, s=cv.ridge$lambda.min)>.5)
#install.packages(c("e1071", "caret", "e1071")
library(caret)
library(ggplot2)
library(lattice)
library(e1071)
confusionMatrix(table(y_test, y_pred), mode="everything")
```
Como vemos , el modelo Ridge nos proporciona una adecuación Accuracy de 0,92 y sobre el test nos da 86 verdaderos positivos,6 verdaderos negativos y 8 falsos positivos .

5. Aporta los *log odds* de las variables predictoras sobre la variable objetivo.

Como sabemos,que un factor tenga por coeficiente β significa que por cada unidad de aumento de ese factor se verifica que la probabilidad de éxito se multiplique por e^(Beta) , por ejemplo , si el coeficiente de una variable es 0.1 en la descripción de la regresión logística , entonces el aumento de probabilidad de la variable respuesta aumenta en un 10.51%

De este modo, se puede interpretar la influencia de los factores en la probabilidad final de que la variable binaria sea un 1.

Lo hacemos en nuestro caso :
```{r}
exp( coef(cv.ridge))
```

6. Si por cada verdadero positivo ganamos 100e y por cada falso positivo perdemos 20e. ¿ Qué rentabilidad aporta aplicar este modelo?

Definimos la fórmula de la rentabilidad pedida en el enunciado y obtenemos los datos que necesitamos de la matriz de confusión de nuestro test .

```{r}
M<-as.matrix(confusionMatrix(table(y_test, y_pred), mode="everything"))
Rentabilidad=100*M[1,1]-20*M[2,1]
Rentabilidad
```
Como podemos observar , la rentabilidad de aplicar este modelo a nuestro test sería de 8440€.


MODELO DE REGRESIÓN LOGÍSTICA CON REGULARIZACIÓN LASSO :

Cargamos la librería glmnet para estos pasos y programamos el modelo a realizar estableciendo medida "AUC" y con dataset de entrenamiento.Como sabemos,en el modelo logístico con regularización Lasso , el valor natural de los coeficientes es 0, penalizando la adjudicación de valor en cada uno. Sin embargo, a diferencia de Ridge y por la construcción de esta regularización, este modelo establece coeficientes finales con valor nulo (los que no son importantes o llamados Huecos).

```{r}
cv.lasso <- cv.glmnet(X_train, y_train, family='binomial', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='auc')
# Resultados
plot(cv.lasso)
#este es el mejor valor de lambda
cv.lasso$lambda.min
#este es el valor del error que se estima para ese valor lambda mínimo dado en MSE
max(cv.lasso$cvm)
```
Vemos los coeficientes del modelo obtenido:

```{r}
coef(cv.lasso, s=cv.lasso$lambda.min)
```
Como podemos observar , además de la variable intercepto , solo las variables A9 y A10 (en menor medida) parecen tener relevancia a tenor de los coeficientes obtenidos en el modelo mientras que las variables A1,A3,A5,A7 y A13 no tienen ninguna relevancia (huecos).

Damos métricas para los valores test y obtenemos la correspondiente matriz de confusión para dichos valores:

```{r}
y_pred <- as.numeric(predict.glmnet(cv.lasso$glmnet.fit, newx=X_test, s=cv.lasso$lambda.min)>.5)
#install.packages(c("e1071", "caret", "e1071")
library(caret)
library(ggplot2)
library(lattice)
library(e1071)
confusionMatrix(table(y_test, y_pred), mode="everything")
```
Como vemos , el modelo Lasso nos proporciona una adecuación Accuracy de 0,91 y sobre el test nos da 85 verdaderos positivos,6 verdaderos negativos,8 falsos positivos y 1 falso negativo.


5. Aporta los *log odds* de las variables predictoras sobre la variable objetivo.

Por lo explicado anteriormente en el modelo Ridge , procedemos de la misma manera:
```{r}
exp( coef(cv.lasso))
```
Como podemos ver , la variable A9 (sobre todo ) y la A10 tienen influencia en el aumento de probabilidad de la variable respuesta .

6. Si por cada verdadero positivo ganamos 100e y por cada falso positivo perdemos 20e. ¿ Qué rentabilidad aporta aplicar este modelo?

Procedemos exactamente igual a como lo hemos hecho con el modelo Ridge.
```{r}
M<-as.matrix(confusionMatrix(table(y_test, y_pred), mode="everything"))
Rentabilidad=100*M[1,1]-20*M[2,1]
Rentabilidad
```
Como podemos observar , la rentabilidad de aplicar este modelo a nuestro test sería de 8340€ , 100€ menos que en el modelo Ridge , por lo que añadido al hecho de que el Ridge tiene un coeficiente superior ( en una centésima ) al modelo Lasso , establecemos como mejor modelo el de Regresión Logística con regularización Ridge.

En cuanto a las variables que parecen influir en la decisión de aprobación de crédito , podemos ver que la variable A9 y , en menor medida , la A10 , parecen ser las más decisivas , por los que podemos aventurar que podrían ser la existencia de falta de pago en anteriores operaciones crediticias y el hecho de tener ingresos regulares procedentes de un salario fijo respectivamente , en mi opinión .






